{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries/packages\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to obtain the latest version of Chrome without needing to manually download the driver and add it to the path\n",
    "browser = Browser(\"chrome\", service=ChromeService(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary used in webpage url generation\n",
    "states = [\n",
    "    {'abbrev': 'MN', 'name': 'Minnesota'},\n",
    "    {'abbrev': 'NY', 'name': 'NewYork'},\n",
    "    {'abbrev': 'IL', 'name': 'Illinois'},\n",
    "    {'abbrev': 'IN', 'name': 'Indiana'},\n",
    "    {'abbrev': 'MI', 'name': 'Michgan'},\n",
    "    {'abbrev': 'OH', 'name': 'Ohio'},\n",
    "    {'abbrev': 'PA', 'name': 'Pennslyvania'},\n",
    "    {'abbrev': 'WI', 'name': 'Wisconsin'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem of page urls \n",
    "base_url = \"https://ecos.fws.gov/ecp/report/species-listings-by-state\"\n",
    "\n",
    "# list used to store scrape results (holds each row_record as shown below)\n",
    "data = []\n",
    "\n",
    "# iterate through all pages\n",
    "for state in states:\n",
    "    url = f\"{base_url}?stateAbbrev={state['abbrev']}&stateName={state['name']}&statusCategory=Listed\"\n",
    "\n",
    "    # visit page in automated browser\n",
    "    browser.visit(url)\n",
    "    print(\"Page visited: \", url)\n",
    "\n",
    "    # modify page so that All results show on the page without pageination\n",
    "    browser.find_by_xpath(\n",
    "        \"//select[@name='species-listings-by-state-report_length']/option[text()='All']\"\n",
    "    ).click()\n",
    "\n",
    "    # grab page html and make into soup object\n",
    "    html_content = browser.html\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "   # base url for each animal link\n",
    "    base_animal_url = \"https://ecos.fws.gov\"\n",
    "    for table_row in soup.find_all(\"tr\"):\n",
    "        # dictionary that will hold info obtained from one table row\n",
    "        row_record = {}\n",
    "\n",
    "        # find link in each row and add to dictionary as key 'url'\n",
    "        row_link = table_row.find(\"a\", href=True)\n",
    "        # filter out some of the other results not related to our goal\n",
    "        if (row_link == None) or (row_link[\"href\"][0:4] == \"http\"):\n",
    "            # skip unwanted selection and move to next iteration in loop\n",
    "            continue\n",
    "        else:\n",
    "            row_record[\"url\"] = base_animal_url + row_link[\"href\"]\n",
    "        \n",
    "        # find animal name in each row and add to dictionary as key 'name'\n",
    "        name = table_row.find(\"td\", class_=\"sorting_2\").text\n",
    "        row_record[\"name\"] = name\n",
    "\n",
    "        # find state name for each page that is scraped and add to dictionary as key 'state'\n",
    "        row_record[\"state\"] = state[\"name\"]\n",
    "        data.append(row_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view results in pandas for easy viewing\n",
    "df = pd.DataFrame(data)\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if you have all the results - inspect scraped page and dataframe to make sure everything was captured\n",
    "\n",
    "# find total records scraped\n",
    "print('total records: ', len(df))\n",
    "\n",
    "# groupby each state and find total entries\n",
    "df.groupby('state')['url'].count()\n",
    "\n",
    "# view one state records and compare webpage \n",
    "df[df['state'] == 'NewYork']\n",
    "\n",
    "# There are duplicate records in New Yor for Piping Plover - Why?\n",
    "# What else might you want to capture and put into this dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using list of dictionaries above that has a dictionary for each animal (row)\n",
    "for animal in data:\n",
    "    browser.visit(animal[\"url\"])\n",
    "    # slow down scrape to server does see rapid hits coming from one IP\n",
    "    time.sleep(1)\n",
    "\n",
    "    # extract browser html\n",
    "    html_content = browser.html\n",
    "\n",
    "    # create soup object\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # attempt to search for images on each animal page but if none just record it as none\n",
    "    try:\n",
    "        image = soup.find(\"img\", class_=\"imageSize\")[\"src\"]\n",
    "    except:\n",
    "        image = None\n",
    "\n",
    "    # attempt to search for endangered animals on each animal page but if none just record it as none\n",
    "    try:\n",
    "        end_status = soup.find(\"span\", class_=\"listingEnd\").text\n",
    "    except:\n",
    "        end_status = None\n",
    "\n",
    "    # attempt to search for threatened animals on each animal page but if none just record it as none\n",
    "    try:\n",
    "        threat_status = soup.find(\"span\", class_=\"listingThreat\").text\n",
    "    except:\n",
    "        threat_status = None\n",
    "\n",
    "    # attempt to search for paragraphs of info on each animal page but if none just record it as none\n",
    "    try:\n",
    "        general_info = soup.find(\"div\", {\"id\": \"j-general-info\"}).text\n",
    "    except:\n",
    "        general_info = None\n",
    "\n",
    "    # store all info for this particular page into the dictionary originally accessed (where we got the link)\n",
    "    animal[\"image_url\"] = image\n",
    "    animal[\"endangered\"] = end_status\n",
    "    animal[\"threatened\"] = threat_status\n",
    "    animal[\"description\"] = general_info\n",
    "\n",
    "# does the endangered/threatened content need collect from this page?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view complete dataframe\n",
    "df = pd.DataFrame(data)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect df\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view number of nan scraped values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show missing image url and check to see if the image is missing or if it is a code issue\n",
    "df[df['image_url'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check of the images not missing to see why there are so few unique image urls\n",
    "no_na_df = df[~df[\"image_url\"].isna()]\n",
    "no_na_df[no_na_df[\"image_url\"].duplicated(keep=False)].sort_values(by=\"image_url\").head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move full dataset to csv file\n",
    "df.to_csv('great_lakes_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view a deduped df\n",
    "df.drop_duplicates(subset=['url'], inplace=True, keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('output.json', orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
